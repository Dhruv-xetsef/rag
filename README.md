
# ğŸŒŸ **RAG & Chatbot Playground â€” LangChain â€¢ Groq â€¢ Open-Source LLMs**

An interactive collection of Retrieval-Augmented Generation pipelines, chatbots, vector search engines, and LLM applications built with **LangChain**, **FAISS**, **FastAPI**, and **Streamlit**.

Supports local & API-based LLMs like **Mistral**, **Gemma**, **Phi**, **DeepSeek**, and more.
Fetches knowledge from **PDFs, text files, websites, and structured/unstructured data sources**.

---

## ğŸš€ **Features**

* **RAG (Retrieval-Augmented Generation)** with FAISS, HuggingFace, and LangChain
* **Multi-model support:** Groq, OpenAI-compatible APIs, Mistral, DeepSeek, Gemma, Phi, Llama
* **Document ingestion:** PDF, TXT, Web Scraping, Wikipedia, ArXiv, HTML, custom text
* **Embeddings:** Sentence Transformers, LangChain embeddings
* **Vector Databases:** FAISS CPU
* **Interactive frontends:** Streamlit UI + FastAPI backend via LangServe
* **Environment file support:** `.env` for API keys (ignored by Git)
* **BeautifulSoup-based web scraping**
* **Chat history, memory components, and tools integrations**
* **Fully modular pipeline for training, retrieval & inference**

---

## ğŸ“ **Project Structure (Example)**

```
ğŸ“¦ rag-chatbot-project
 â”£ ğŸ“œ app.py                # Streamlit UI
 â”£ ğŸ“œ server.py             # FastAPI / LangServe backend
 â”£ ğŸ“œ rag_pipeline.py       # Core RAG pipeline
 â”£ ğŸ“œ ingest.py             # PDF / TXT / Web ingestion
 â”£ ğŸ“œ vectorstore.py        # FAISS index builder
 â”£ ğŸ“œ models.py             # Model loaders (Groq, HF, OpenAI)
 â”£ ğŸ“œ utils.py              # Helpers
 â”£ ğŸ“œ requirements.txt
 â”£ ğŸ“œ .gitignore
 â”— ğŸ“œ README.md
```

---

## ğŸ§  **LLM Support**

This repo supports:

### ğŸ”¥ **Open-Source Local Models**

* **Mistral**
* **Gemma**
* **Phi / Phi-2 / Phi-3**
* **DeepSeek**
* **Llama 2 / 3**
* **Mixtral**
* **Other HF models via `transformers` or `sentence-transformers`**

### âš¡ **API-Based Models**

* **Groq API** (blazing fast Llama/Gemma)
* **OpenAI-compatible models** (via LangChain)
* **LangChainHub hosted prompts**

---

## ğŸ“š **Document & Data Sources**

You can load knowledge from:

* ğŸ“ **TXT files**
* ğŸ“„ **PDF documents** (via PyPDF)
* ğŸŒ **Web pages** (BeautifulSoup + Requests)
* ğŸ“˜ **Wikipedia**
* ğŸ“• **ArXiv papers**
* ğŸ§µ Custom datasets
* ğŸ—‚ï¸ Mixed formats (HTML, markdown, transcripts, notes)

Everything gets chunked â†’ embedded â†’ stored in FAISS â†’ retrieved during chat.

---

## ğŸ® **How to Run**

### 1ï¸âƒ£ **Install dependencies**

```
pip install -r requirements.txt
```

### 2ï¸âƒ£ **Create your `.env` file**

```
OPENAI_API_KEY=
GROQ_API_KEY=
HUGGINGFACEHUB_API_TOKEN=
```

(The `.env` is automatically git-ignored.)

### 3ï¸âƒ£ **Ingest your data**

```
python ingest.py --path documents/my.pdf
```

### 4ï¸âƒ£ **Start the API server**

```
uvicorn server:app --reload
```

### 5ï¸âƒ£ **Launch the Streamlit frontend**

```
streamlit run app.py
```

---

## ğŸ— **Tech Stack**

### ğŸ”¹ Core Libraries

* **LangChain**
* **LangChain OpenAI**
* **LangChain Groq**
* **LangChain Community**
* **LangChainHub**

### ğŸ”¹ Vector Search

* **FAISS CPU**

### ğŸ”¹ Backend

* **FastAPI**
* **LangServe**
* **Uvicorn**
* **SSE Starlette (streaming responses)**

### ğŸ”¹ Frontend

* **Streamlit**

### ğŸ”¹ Utilities

* **python-dotenv**
* **BeautifulSoup4 (bs4)**
* **PyPDF**
* **Sentence Transformers**
* **Cassio (optional, Cassandra integrations)**

---

## ğŸ“¦ **requirements.txt**

Includes everything you provided:

```
langchain-openai
langchain-core
python-dotenv
streamlit
langchain-community
langserve
fastapi
uvicorn
sse_starlette
bs4
pypdf
faiss-cpu
wikipedia
beautifulsoup4
cassio
arxiv
langchainhub
langchain
groq
langchain-groq
sentence_transformers
```

---

## ğŸ§ª **RAG Workflow Overview**

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Documents    â”‚  (PDF / TXT / Web / Wiki)
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Chunk & Embed     â”‚  (SentenceTransformers)
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  FAISS Index      â”‚  (Vector DB)
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Retriever + LLM       â”‚  (Groq / OpenAI / OSS models)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒˆ **Planned Enhancements**

* Multi-source hybrid search (web + PDF + knowledge base)
* Chat memory with long-term vector persistence
* Agents with tools & browser search
* GPU-accelerated local inference with VLLM / Ollama
* Conversation analytics dashboard

---


---

# ğŸŒˆ **1. Project Banner (Markdown)**


```md
<h1 align="center">ğŸ”® RAG & Chatbot Playground</h1>
<h3 align="center">LangChain â€¢ Groq â€¢ FAISS â€¢ Streamlit â€¢ Open-Source LLMs</h3>

<p align="center">
  Build fast, smart, modular AI systems using RAG, vector search, and modern LLM APIs.
</p>

<p align="center">
  <img src="https://img.shields.io/badge/LangChain-RAG-blue" />
  <img src="https://img.shields.io/badge/FAISS-VectorDB-green" />
  <img src="https://img.shields.io/badge/Groq-LLMs-orange" />
  <img src="https://img.shields.io/badge/Streamlit-UI-red" />
  <img src="https://img.shields.io/badge/OpenSource-Models-yellow" />
</p>
```

---



```md
## ğŸ¨ Streamlit Interface Preview

> *(Add your screenshot into the repo at `images/ui.png` or `images/app.png`)*

![Streamlit UI](images/ui.png)
```



---

# ğŸ§  **3. Architecture Diagram (Mermaid + ASCII)**



````md
## ğŸ§¬ System Architecture

```mermaid
flowchart TD
    A[User Query] --> B[Retriever]
    B --> C[FAISS Vector Store]
    B --> D[Document Loaders<br/>(PDF, TXT, Web, Wiki, ArXiv)]
    D --> E[Text Splitter]
    E --> F[Embeddings<br/>(Sentence Transformers)]
    F --> C
    B --> G[LLM Model<br/>(Groq / Mistral / Gemma / Phi)]
    G --> H[Final Response]
````

```

### **ASCII Diagram (if viewer doesnâ€™t support Mermaid)**

```

User Query
|
Retriever ----- Document Loaders (PDF/TXT/Web/Wiki)
|                     |
|                 Text Splitter
|                     |
FAISS Vector Store <--- Embeddings (Sentence Transformers)
|
LLM Model (Groq / Mistral / Gemma / Phi)
|
Final Response

```

---

# ğŸ” **4. `.env.example` file**

Create this file in your repo root:

```

# API Keys

OPENAI_API_KEY=
GROQ_API_KEY=
HUGGINGFACEHUB_API_TOKEN=

# Optional

LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=

````

Your `.env` stays hidden (thanks to `.gitignore`).  
`.env.example` tells users what variables they need.

---

# âš¡ **5. QuickStart Section (Clean & Simple)**

Add this below â€œInstallationâ€:

```md
## âš¡ QuickStart

```bash
git clone https://github.com/<username>/<repo>.git
cd <repo>

# 1. Install dependencies
pip install -r requirements.txt

# 2. Copy the example environment file
cp .env.example .env
# Add your API keys in .env

# 3. Ingest documents
python ingest.py --path data/my.pdf

# 4. Start API server
uvicorn server:app --reload

# 5. Launch Streamlit application
streamlit run app.py
````

````

---

# ğŸ”¥ **6. Optional Add-On: â€œModels You Can Useâ€ Section**

```md
## ğŸ¤– Supported Open-Source LLMs

| Model | Mode | Provider |
|-------|--------|----------|
| **Mistral-7B** | Local / API | HF / Replicate |
| **Gemma 2B/7B** | Local / API | Groq / HF |
| **Phi-2 / Phi-3** | Local | HF |
| **DeepSeek R1** | API | Deepseek |
| **Llama 3** | API / Local | Groq / HF |
````


## ğŸ¤ **Contributing**

PRs are welcome!
This repo is built to be modular â€” add new ingestion methods, new models, or new RAG pipelines.

---

## ğŸ‘‘ **Author**

**Dhruv (xetsef)**
AI â€¢ RAG Systems â€¢ LLM Engineering
Crafting intelligent systems with a little spark âœ¨

